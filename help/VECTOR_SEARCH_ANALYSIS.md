# Vector Search Analysis & Improvement Recommendations

## Executive Summary

**Good News**: Your application **already embeds user queries** for vector search! The implementation follows best practices by using the same embedding model (`all-MiniLM-L6-v2`) for both document indexing and query embedding.

## Current Implementation

### What's Working Well âœ…

1. **Query Embedding is Enabled** (line 83 in `rag_service.py`):
   ```python
   query_embedding = self._embedder.encode(query).tolist()
   ```

2. **Consistent Embedding Model**: Both documents and queries use the same SentenceTransformer model (`all-MiniLM-L6-v2`)

3. **Cosine Similarity**: Using cosine distance metric (line 47 in `rag_service.py`)
   ```python
   metadata={"hnsw:space": "cosine"}
   ```

4. **Proper Vector Search Flow**:
   - User query â†’ Embedded â†’ Search ChromaDB â†’ Return top-k similar documents
   - This is the correct semantic search approach

## Potential Improvements

While your implementation is solid, here are some evidence-based improvements you can consider:

### 1. **Query Expansion & Reformulation** ðŸ”¥ High Impact

**Problem**: Users often phrase questions differently than how information is stored in documents.

**Solution**: Enhance queries before embedding:

```python
def expand_query(self, query: str, use_llm: bool = True) -> str:
    """Expand query with synonyms or rephrase for better retrieval."""
    if use_llm:
        # Use LLM to rephrase/expand (e.g., HyDE technique)
        expansion_prompt = f"""Given this question: "{query}"
        
Generate 2-3 alternative phrasings or related questions that capture the same intent:"""
        # Get LLM to generate alternatives
        # Then combine original + alternatives
    else:
        # Simple keyword expansion
        # Add synonyms, related terms
    return expanded_query
```

**Expected Improvement**: 15-30% better recall in many RAG systems

### 2. **Upgrade Embedding Model** ðŸ”¥ High Impact

**Current**: `all-MiniLM-L6-v2` (384 dimensions, fast but basic)

**Better Options**:

| Model | Dimensions | Performance | Speed | Use Case |
|-------|-----------|-------------|-------|----------|
| `all-mpnet-base-v2` | 768 | Better (+10-15%) | Slower | General purpose |
| `BAAI/bge-small-en-v1.5` | 384 | Better (+5-10%) | Similar | English docs |
| `BAAI/bge-base-en-v1.5` | 768 | Best (+15-20%) | Slower | Production quality |
| `intfloat/e5-base-v2` | 768 | Excellent | Moderate | Latest research |

**Implementation**:
```python
# In config.py
embedding_model: str = "BAAI/bge-base-en-v1.5"  # or "all-mpnet-base-v2"
```

**âš ï¸ Important**: Changing models requires re-embedding ALL documents!

### 3. **Multi-Query Retrieval** ðŸ”¥ Medium-High Impact

Generate multiple versions of the query and retrieve for each:

```python
def multi_query_search(self, query: str, n_results: int = 3) -> list[dict]:
    """Search using multiple query variations."""
    # Generate query variations
    queries = [
        query,
        self._rephrase_query(query, style="formal"),
        self._rephrase_query(query, style="casual"),
    ]
    
    all_results = []
    seen_ids = set()
    
    for q in queries:
        results = self.search(q, n_results=n_results)
        for doc in results:
            if doc['id'] not in seen_ids:
                all_results.append(doc)
                seen_ids.add(doc['id'])
    
    # Re-rank by combining scores
    return self._rerank_results(all_results)[:n_results]
```

### 4. **Hybrid Search (BM25 + Vector)** ðŸ”¥ High Impact

Combine keyword search (BM25) with semantic search:

```python
def hybrid_search(self, query: str, n_results: int = 3, alpha: float = 0.5) -> list[dict]:
    """
    Combine BM25 (keyword) and vector (semantic) search.
    alpha: weight for vector search (0=pure BM25, 1=pure vector)
    """
    # Get vector search results
    vector_results = self.search(query, n_results=n_results*2)
    
    # Get BM25 results (would need to add BM25 index)
    bm25_results = self._bm25_search(query, n_results=n_results*2)
    
    # Combine and re-rank using Reciprocal Rank Fusion (RRF)
    combined = self._reciprocal_rank_fusion(
        [vector_results, bm25_results],
        weights=[alpha, 1-alpha]
    )
    
    return combined[:n_results]
```

**Expected Improvement**: 20-40% better performance on keyword-heavy queries

### 5. **Query Preprocessing** ðŸ”¥ Low-Medium Impact

Clean and optimize queries before embedding:

```python
def preprocess_query(self, query: str) -> str:
    """Clean and optimize query for better matching."""
    # Remove special characters
    query = re.sub(r'[^\w\s]', ' ', query)
    
    # Normalize whitespace
    query = ' '.join(query.split())
    
    # Optional: Add context prefix for better embedding
    # Some models (like E5) benefit from instruction prefixes
    if self.embedding_model.startswith('intfloat/e5'):
        query = f"query: {query}"
    elif self.embedding_model.startswith('BAAI/bge'):
        query = f"Represent this sentence for searching relevant passages: {query}"
    
    return query
```

### 6. **Adjust n_results Dynamically** ðŸ”¥ Low Impact

```python
def adaptive_search(self, query: str, min_similarity: float = 0.5) -> list[dict]:
    """Return results above similarity threshold."""
    # Search with larger k
    results = self.search(query, n_results=10)
    
    # Filter by similarity score (distance < threshold)
    # Note: ChromaDB returns cosine distance, where lower is better
    filtered = [
        doc for doc in results 
        if doc['distance'] < (1 - min_similarity)  # Convert similarity to distance
    ]
    
    return filtered[:5]  # Return top 5 above threshold
```

### 7. **Add Query Instructions (Asymmetric Search)** ðŸ”¥ Medium Impact

Some modern embedding models perform better with query instructions:

```python
def encode_with_instruction(self, text: str, is_query: bool = True) -> list:
    """Encode with model-specific instructions."""
    if self.embedding_model.startswith('BAAI/bge'):
        if is_query:
            text = f"Represent this sentence for searching relevant passages: {text}"
        # Documents don't need instruction
    elif self.embedding_model.startswith('intfloat/e5'):
        prefix = "query: " if is_query else "passage: "
        text = f"{prefix}{text}"
    
    return self._embedder.encode(text).tolist()
```

## Recommended Action Plan

### Phase 1: Quick Wins (1-2 hours)
1. âœ… Add query preprocessing (#5)
2. âœ… Implement adaptive search with similarity threshold (#6)
3. âœ… Add query instructions for better embeddings (#7)

### Phase 2: Moderate Improvements (2-4 hours)
4. ðŸ”„ Upgrade embedding model to `BAAI/bge-base-en-v1.5` or `all-mpnet-base-v2` (#2)
   - **Note**: Requires re-embedding all documents
5. ðŸ”„ Implement multi-query retrieval (#3)

### Phase 3: Advanced Features (4-8 hours)
6. ðŸš€ Add hybrid search (BM25 + Vector) (#4)
7. ðŸš€ Implement query expansion with LLM (#1)

## Performance Metrics to Track

Before and after implementing changes, measure:

1. **Retrieval Accuracy**: Are relevant documents in top-k results?
2. **Query Latency**: Time to retrieve documents
3. **Answer Quality**: Does the LLM give better answers with improved context?
4. **User Satisfaction**: Subjective quality assessment

## Testing Strategy

```python
# Create test queries and expected relevant docs
test_cases = [
    {
        "query": "How do I deploy this app?",
        "expected_docs": ["deployment_guide.txt", "docker_setup.md"],
    },
    {
        "query": "What is RAG?",
        "expected_docs": ["README.md", "ai_concepts.md"],
    },
    # Add more test cases
]

# Calculate metrics
for case in test_cases:
    results = rag_service.search(case["query"], n_results=5)
    result_ids = [r['metadata'].get('filename') for r in results]
    
    # Precision@k, Recall@k, MRR, etc.
    precision = calculate_precision(result_ids, case["expected_docs"])
    print(f"Precision@5: {precision}")
```

## Conclusion

Your current implementation is **already using query embeddings correctly**! The recommendations above are optimizations that can provide incremental improvements ranging from 5-40% depending on your specific use case and data.

**Most Impactful Changes** (in order):
1. ðŸ¥‡ Hybrid Search (BM25 + Vector) - Best for keyword-heavy queries
2. ðŸ¥ˆ Better Embedding Model - Universal improvement
3. ðŸ¥‰ Multi-Query Retrieval - Better recall
4. Query Expansion with LLM - Domain-specific improvement

Start with Phase 1 quick wins, measure the impact, then decide if Phase 2 and 3 are worth the effort for your use case.
