"""Chat router with streaming support."""
import json
import logging
from typing import AsyncGenerator

from fastapi import APIRouter, HTTPException, File, UploadFile
from fastapi.responses import StreamingResponse
from sse_starlette.sse import EventSourceResponse

from app.models.schemas import (
    ChatRequest,
    ChatResponse,
    DocumentRequest,
    DocumentResponse,
    DocumentUploadResponse,
    DocumentListResponse,
)
from app.services.graph_service import graph_service
from app.services.rag_service import rag_service
from app.services.llm_service import llm_service
from app.services.document_service import document_service

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api", tags=["chat"])


@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest) -> ChatResponse:
    """
    Non-streaming chat endpoint.
    
    Processes the message through the LangGraph workflow and returns
    the complete response. Supports multi-hop reasoning for complex queries.
    """
    if not llm_service.is_loaded:
        raise HTTPException(
            status_code=503,
            detail="LLM model not loaded. Please ensure a model is available."
        )
    
    try:
        result = graph_service.chat(
            message=request.message,
            conversation_id=request.conversation_id,
            use_rag=request.use_rag,
            use_reasoning=request.use_reasoning,
            max_hops=request.max_hops,
        )
        
        return ChatResponse(
            message=result["message"],
            conversation_id=result["conversation_id"],
            sources=result.get("sources"),
        )
        
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat/reasoning", response_model=ChatResponse)
async def chat_with_reasoning(request: ChatRequest):
    """
    Chat endpoint with full multi-hop reasoning chain.
    
    This endpoint returns the complete reasoning chain showing how
    the answer was derived through multiple reasoning steps.
    """
    if not llm_service.is_loaded:
        raise HTTPException(
            status_code=503,
            detail="LLM model not loaded. Please ensure a model is available."
        )
    
    try:
        # Force reasoning mode and RAG
        result = graph_service.chat(
            message=request.message,
            conversation_id=request.conversation_id,
            use_rag=True,  # Multi-hop requires RAG
            use_reasoning=True,  # Always use reasoning in this endpoint
            max_hops=request.max_hops,
        )
        
        from app.models.schemas import MultiHopChatResponse, ReasoningStepResponse
        
        # Convert reasoning chain to response models
        reasoning_chain = None
        if result.get("reasoning_chain"):
            reasoning_chain = [
                ReasoningStepResponse(**step)
                for step in result["reasoning_chain"]
            ]
        
        return MultiHopChatResponse(
            message=result["message"],
            conversation_id=result["conversation_id"],
            sources=result.get("sources"),
            reasoning_chain=reasoning_chain,
            strategy_used=result.get("strategy_used"),
            needs_multi_hop=bool(reasoning_chain),
        )
        
    except Exception as e:
        logger.error(f"Multi-hop reasoning error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def stream_generator(
    message: str,
    conversation_id: str = None,
    use_rag: bool = False,
    use_reasoning: bool = False,
    max_hops: int = 3,
) -> AsyncGenerator[dict, None]:
    """Generate SSE events for streaming response."""
    try:
        # Send start event
        yield {
            "event": "start",
            "data": json.dumps({"conversation_id": conversation_id or "new"}),
        }
        
        # Stream tokens
        async for token in graph_service.chat_stream(
            message=message,
            conversation_id=conversation_id,
            use_rag=use_rag,
            use_reasoning=use_reasoning,
            max_hops=max_hops,
        ):
            yield {
                "event": "token",
                "data": json.dumps({"token": token}),
            }
        
        # Send end event
        yield {
            "event": "end",
            "data": json.dumps({"status": "complete"}),
        }
        
    except Exception as e:
        logger.error(f"Stream error: {e}")
        yield {
            "event": "error",
            "data": json.dumps({"error": str(e)}),
        }


@router.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    Streaming chat endpoint using Server-Sent Events (SSE).
    
    Streams tokens as they are generated by the LLM, providing
    a real-time chat experience similar to ChatGPT.
    
    Events:
    - start: Initial event with conversation_id
    - token: Individual tokens as they are generated
    - end: Completion event
    - error: Error event if something goes wrong
    """
    if not llm_service.is_loaded:
        raise HTTPException(
            status_code=503,
            detail="LLM model not loaded. Please ensure a model is available."
        )
    
    return EventSourceResponse(
        stream_generator(
            message=request.message,
            conversation_id=request.conversation_id,
            use_rag=request.use_rag,
            use_reasoning=request.use_reasoning,
            max_hops=request.max_hops,
        )
    )


@router.post("/documents", response_model=DocumentResponse)
async def add_document(request: DocumentRequest) -> DocumentResponse:
    """
    Add a document to the RAG knowledge base.
    
    The document will be embedded and stored in ChromaDB for
    later retrieval during RAG-enabled chat.
    """
    try:
        doc_id = rag_service.add_document(
            content=request.content,
            metadata=request.metadata,
        )
        
        return DocumentResponse(
            id=doc_id,
            message=f"Document added successfully. Total documents: {rag_service.get_document_count()}",
        )
        
    except Exception as e:
        logger.error(f"Document upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/documents/count")
async def get_document_count():
    """Get the number of documents in the knowledge base."""
    return {"count": rag_service.get_document_count()}


@router.post("/documents/upload", response_model=DocumentUploadResponse)
async def upload_document_file(file: UploadFile = File(...)):
    """
    Upload a document file for RAG indexing.
    
    Supports PDF, TXT, and MD files. Documents are processed,
    chunked, and stored in ChromaDB. Duplicate documents are
    detected via content hashing and skipped.
    """
    try:
        result = await document_service.upload_document(file)
        
        return DocumentUploadResponse(
            id=result.id,
            filename=result.filename,
            status=result.status,
            message=result.message,
            chunks_created=result.chunks_created,
        )
        
    except Exception as e:
        logger.error(f"File upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/documents", response_model=DocumentListResponse)
async def list_documents():
    """List all uploaded documents."""
    try:
        documents = document_service.list_documents()
        return DocumentListResponse(
            documents=documents,
            total=len(documents),
        )
    except Exception as e:
        logger.error(f"Error listing documents: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/documents/{content_hash}")
async def delete_document(content_hash: str):
    """Delete a document by its content hash."""
    try:
        success = document_service.delete_document(content_hash)
        if not success:
            raise HTTPException(status_code=404, detail="Document not found")
        return {"message": "Document deleted successfully"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting document: {e}")
        raise HTTPException(status_code=500, detail=str(e))

